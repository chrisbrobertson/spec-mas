# =============================================================================
# SPEC-MAS DISTRIBUTED CONFIGURATION
# =============================================================================
# Copy this file to .env and update with your actual values

# =============================================================================
# NODE CONFIGURATION
# =============================================================================

# Mac Studio - Primary code generation node (Ollama)
# Update with your Mac Studio's actual IP address
MAC_STUDIO_HOST=192.168.1.50
MAC_STUDIO_PORT=11434
MAC_STUDIO_ENABLED=true

# DGX Spark - Heavy compute node (Ollama)
# Update with your DGX Spark's actual IP address
DGX_SPARK_HOST=192.168.1.100
DGX_SPARK_PORT=11434
DGX_SPARK_ENABLED=true

# Claude API - Fallback / complex reasoning
# Get your key from https://console.anthropic.com
ANTHROPIC_API_KEY=sk-ant-api03-your-key-here
CLAUDE_MODEL=claude-sonnet-4-20250514
CLAUDE_ENABLED=true
CLAUDE_FALLBACK_ONLY=true

# =============================================================================
# ROUTING CONFIGURATION
# =============================================================================

# Strategy: cost-optimized | speed-optimized | quality-optimized
ROUTING_STRATEGY=cost-optimized

# Prefer local nodes over cloud (recommended for cost savings)
PREFER_LOCAL=true

# Max retries on local nodes before falling back to Claude
MAX_LOCAL_RETRIES=2

# Timeout for node health checks (ms)
HEALTH_CHECK_TIMEOUT=5000

# =============================================================================
# AI PROVIDER SETTINGS
# =============================================================================

# Use distributed routing (new multi-backend system)
AI_PROVIDER=distributed

# Default model settings for local nodes
AI_TEMPERATURE=0.2
AI_TOP_P=0.9
AI_MAX_TOKENS=8192

# =============================================================================
# COST TRACKING
# =============================================================================

# Estimated costs for tracking purposes
# Local inference is free, these are for reference
LOCAL_INPUT_COST=0.00
LOCAL_OUTPUT_COST=0.00

# Claude costs (per 1M tokens) - for budget tracking
CLAUDE_INPUT_COST=3.00
CLAUDE_OUTPUT_COST=15.00

# Budget limits (Claude API only)
SPECMAS_BUDGET_WARNING=50.00
SPECMAS_BUDGET_LIMIT=150.00

# Track local inference "costs" for comparison
TRACK_LOCAL_COSTS=true

# =============================================================================
# OUTPUT CONFIGURATION
# =============================================================================

OUTPUT_DIR=implementation-output
REPORTS_DIR=.specmas/reports
TESTS_DIR=tests/generated

# =============================================================================
# LOGGING
# =============================================================================

LOG_LEVEL=info
LOG_AI_CALLS=true
VERBOSE=false

# =============================================================================
# GIT INTEGRATION
# =============================================================================

GIT_AUTO_COMMIT=true
GIT_AUTO_BRANCH=true
GIT_BRANCH_PREFIX=feat/spec-

# =============================================================================
# PIPELINE SETTINGS
# =============================================================================

# Enable checkpointing for long-running pipelines
PIPELINE_CHECKPOINT_ENABLED=true

# Auto-resume from last checkpoint if interrupted
PIPELINE_AUTO_RESUME=true

# Run implementation tasks in parallel (uses more resources)
PIPELINE_PARALLEL_EXECUTION=false
